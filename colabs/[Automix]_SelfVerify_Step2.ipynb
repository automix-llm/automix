{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/automix-llm/automix/blob/main/colabs/%5BAutomix%5D_SelfVerify_Step2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZpOXtkD-8oYZ"
      },
      "source": [
        "# Run verification on the llama2-13b/70b outputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LCohyStx1JqL"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from tqdm import tqdm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWCPZ_JV8uiW"
      },
      "outputs": [],
      "source": [
        "# get these outputs from https://drive.google.com/file/d/1dhyt7UuYumk9Gae9eJ_mpTVrLeSTuRht/view?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TrDKNY-1Nt9"
      },
      "outputs": [],
      "source": [
        "llama2_outputs = pd.read_json(\"data/automix_llamapair_outputs.jsonl\", lines=True, orient=\"records\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "1Chiv4DP28Yz",
        "outputId": "a2bb8aba-cf3d-4453-e7de-960b80759fd7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>llama13b_pred_ans</th>\n",
              "      <th>llama70b_pred_ans</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>What is another term for a trashcan in an offi...</td>\n",
              "      <td>garbage can.</td>\n",
              "      <td>wastebasket.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question llama13b_pred_ans  \\\n",
              "0  What is another term for a trashcan in an offi...      garbage can.   \n",
              "\n",
              "  llama70b_pred_ans  \n",
              "0      wastebasket.  "
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "llama2_outputs[['question', 'llama13b_pred_ans', 'llama70b_pred_ans']].head(1)\n",
        "# we have outputs from the two llama models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2efylito2_cK"
      },
      "source": [
        "### OpenAI Query"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpCw0K9e3Bto"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "openai.api_key = \"EMPTY\"\n",
        "openai.api_base = \"http://pitt.lti.cs.cmu.edu:8003/v1\"\n",
        "# ^ please update your URLs, see https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html\n",
        "\n",
        "engine = \"meta-llama/Llama-2-13b-hf\"\n",
        "\n",
        "def call_openai_api(prompt, engine_name: str = engine, temperature=0.0, n=1, stop='\\n', max_tokens: int = 100):\n",
        "    BATCH_SIZE = 32\n",
        "\n",
        "    all_responses = []\n",
        "    orig_n = n\n",
        "\n",
        "    try:\n",
        "        while n > 0:\n",
        "            current_batch_size = min(n, BATCH_SIZE)\n",
        "            response = openai.Completion.create(\n",
        "                        model=engine_name,\n",
        "                        prompt=prompt,\n",
        "                        temperature=temperature,\n",
        "                        max_tokens=max_tokens,\n",
        "                        n=current_batch_size,\n",
        "                        stop=stop,\n",
        "                    )\n",
        "\n",
        "            all_responses.extend([choice['text'] for choice in response['choices']])\n",
        "            n -= current_batch_size\n",
        "\n",
        "        return all_responses if orig_n > 1 else all_responses[0]\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1UkBG6g2y2T"
      },
      "outputs": [],
      "source": [
        "from functools import partial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2b0SJFbJBAB"
      },
      "source": [
        "### Run verification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l0vm6-Y9wk3X"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-hf\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t6iU1-xQJCT2"
      },
      "outputs": [],
      "source": [
        "verifier_prompt = \"\"\"Context: The manuscript, discovered in 1980 in a dusty attic, turned out to be a lost work of Shakespeare.\n",
        "\n",
        "Question: Whose lost work was discovered in a dusty attic in 1980?\n",
        "\n",
        "AI Generated Answer: Shakespeare\n",
        "\n",
        "Instruction: Your task is to evaluate if the AI Generated Answer is correct, based on the provided context and question. Provide the judgement and reasoning for each case. Choose between Correct or Incorrect.\n",
        "\n",
        "Evaluation: The context specifically mentions that a lost work of Shakespeare was discovered in 1980 in a dusty attic.\n",
        "\n",
        "Verification Decision: The AI generated answer is Correct.\n",
        "\n",
        "---\n",
        "\n",
        "Context: The celestial event, known as the Pink Moon, is unique to the month of April and has cultural significance in many indigenous tribes.\n",
        "\n",
        "Question: In which month does the celestial event, the Pink Moon, occur?\n",
        "\n",
        "AI Generated Answer: July\n",
        "\n",
        "Instruction: Your task is to evaluate if the AI Generated Answer is correct, based on the provided context and question. Provide the judgement and reasoning for each case. Choose between Correct or Incorrect.\n",
        "\n",
        "Evaluation: The context clearly states that the Pink Moon is unique to the month of April.\n",
        "\n",
        "Verification Decision: The AI generated answer is Incorrect.\n",
        "\n",
        "---\n",
        "\n",
        "Context: The Mona Lisa, housed in the Louvre Museum, is believed to be a portrait of Lisa Gherardini, painted by Leonardo da Vinci in the early 16th century.\n",
        "\n",
        "Question: Who is believed to have painted the Mona Lisa in the early 16th century?\n",
        "\n",
        "AI Generated Answer: Vincent van Gogh\n",
        "\n",
        "Instruction: Your task is to evaluate if the AI Generated Answer is correct, based on the provided context and question. Provide the judgement and reasoning for each case. Choose between Correct or Incorrect.\n",
        "\n",
        "Evaluation: The context specifies that the Mona Lisa was painted by Leonardo da Vinci in the early 16th century.\n",
        "\n",
        "Verification Decision: The AI generated answer is Incorrect.\n",
        "\n",
        "---\n",
        "\n",
        "Context: The planet Kepler-442b, located 1,100 light-years away, is one of the most Earth-like planets ever discovered, having a similar size and orbiting within its star's habitable zone.\n",
        "\n",
        "Question: How far away is the planet Kepler-442b?\n",
        "\n",
        "AI Generated Answer: 1,100 light-years\n",
        "\n",
        "Instruction: Your task is to evaluate if the AI Generated Answer is correct, based on the provided context and question. Provide the judgement and reasoning for each case. Choose between Correct or Incorrect.\n",
        "\n",
        "Evaluation: The context states that Kepler-442b is located 1,100 light-years away.\n",
        "\n",
        "Verification Decision: The AI generated answer is Correct.\n",
        "\n",
        "---\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "AI Generated Answer: {generated_answer}\n",
        "\n",
        "Instruction: Your task is to evaluate if the AI Generated Answer is correct, based on the provided context and question. Provide the judgement and reasoning for each case. Choose between Correct or Incorrect.\n",
        "\n",
        "Evaluation:\"\"\"\n",
        "\n",
        "\n",
        "def make_verifier_input(row):\n",
        "  generated_ans = row[\"generated_answer\"].strip()\n",
        "  return verifier_prompt.format(context=row[\"base_context\"], question=row[\"question\"], generated_answer=generated_ans)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yNtX-4t1TFGa"
      },
      "outputs": [],
      "source": [
        "def make_verifier_input(context, question, generated_answer):\n",
        "    # Create the prompt\n",
        "    prompt_text = verifier_prompt.format(context=context, question=question, generated_answer=generated_answer)\n",
        "\n",
        "    # words =\n",
        "    # Tokenize the prompt\n",
        "    tokens = tokenizer.tokenize(prompt_text)\n",
        "\n",
        "    # Check if tokens exceed the limit\n",
        "    if len(tokens) > 3950:\n",
        "        # Truncate tokens from the left\n",
        "        tokens = tokens[-3950:]\n",
        "\n",
        "        # Convert tokens back to text\n",
        "        truncated_prompt = tokenizer.convert_tokens_to_string(tokens)\n",
        "    else:\n",
        "        truncated_prompt = prompt_text\n",
        "\n",
        "    return truncated_prompt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-J9Te6tTFZ5"
      },
      "outputs": [],
      "source": [
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from functools import partial\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_verification(\n",
        "    df,\n",
        "    ans_col: str,\n",
        "    temperature: float = 1.0,\n",
        "    n: int = 8,\n",
        "    stop: str = '---',\n",
        "    max_tokens: int = 250,\n",
        "    max_workers: int = 32,\n",
        "):\n",
        "    \"\"\"\n",
        "    Runs verification on the input dataframe `df` using concurrent futures and tqdm for progress tracking.\n",
        "\n",
        "    Parameters:\n",
        "    - df: Input dataframe\n",
        "    - ans_col: Column name in df which contains answers\n",
        "    - temperature: Temperature parameter for the call_openai_api function\n",
        "    - n: number of verification samples to draw\n",
        "    - stop: Stop parameter for the call_openai_api function\n",
        "    - max_tokens: Maximum number of tokens to generate\n",
        "    - max_workers: Number of parallel calls to make to the language model\n",
        "\n",
        "    Returns:\n",
        "    - results: Results from the verification\n",
        "    \"\"\"\n",
        "    verifier_inputs = df.apply(\n",
        "        lambda row: make_verifier_input(row[\"base_ctx\"], row[\"question\"], row[ans_col]),\n",
        "        axis=1,\n",
        "    )\n",
        "    verifier_call = partial(\n",
        "        call_openai_api, temperature=temperature, n=n, stop=stop, max_tokens=max_tokens\n",
        "    )\n",
        "\n",
        "    print(\"Inputs prepared, starting verification now.\")\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
        "        results = list(\n",
        "            tqdm(executor.map(verifier_call, verifier_inputs), total=df.shape[0])\n",
        "        )\n",
        "    return results\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGyIn9MCTHun"
      },
      "outputs": [],
      "source": [
        "ver13b = run_verification(llama2_outputs, 'llama13b_pred_ans', max_workers=32, n=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5TDpn7u63w0",
        "outputId": "576f3583-5d19-477f-d76e-e4fbd1cffdbb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "len(ver13b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IhmMNCeGweH-"
      },
      "outputs": [],
      "source": [
        "train['llama13b_ver'] = ver13b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nelVq7YIfI2L"
      },
      "outputs": [],
      "source": [
        "def compute_fraction_correct(lst):\n",
        "    total_valid = sum([1 for item in lst if \"the ai generated answer is\" in item.lower()])\n",
        "    if total_valid == 0:\n",
        "        return 0\n",
        "    correct_count = sum([1 for item in lst if \"the ai generated answer is correct\" in item.lower()])\n",
        "    return correct_count / total_valid\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oxaL3Hci-xXR"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP9wI4mVOGuKzj3tGpVlXXt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
