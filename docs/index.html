<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="AutoMix: Automatically Mixing Language Models">
  <meta name="keywords" content="reasoning, nlp, LLMs, self-verification, meta-verification, GOFAI">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>AutoMix: Automatically Mixing Language Models</title>

  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());
    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">
  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  <link rel="stylesheet" href="./static/css/prism.css"/>
  <link rel="stylesheet" href="plugins/line-highlight/prism-line-highlight.css" data-noprefix="">
  <link rel="stylesheet" href="plugins/diff-highlight/prism-diff-highlight.css" data-noprefix="">

  <script src="./static/js/prism.js"></script>
  <script src="static/js/prism-diff-highlight.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title"><br>AutoMix:<br> Automatically Mixing Language Models</h1>
          <div class="is-size-5 publication-authors"> 
            <span class="author-block"> 
              <a href="https://pranjal2041.github.io">Pranjal Aggarwal</a><sup>‚ô¢</sup><sup>*</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://amanmadaan.github.io/">Aman Madaan</a><sup>‚ô†</sup><sup>*</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://sites.google.com/view/ankitsanand/home">Ankit Anand</a><sup>‚Ä°</sup>, 
            </span>
            <span class="author-block"> 
              <a href="">Srividya Pranavi Potharaju</a><sup>‚Ä†</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://swarooprm.github.io/">Swaroop Mishra</a><sup>‚Ä°</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://shaoxia57.github.io/">Pei Zhou</a><sup>‚ô£</sup>, 
            </span>
            <span class="author-block"> 
              <a href="">Aditya Gupta</a>, 
            </span>
            <span class="author-block"> 
              <a href="">Dheeraj Rajagopal</a><sup>‚Ä†</sup>, 
            </span>
            <span class="author-block"> 
              <a href="">Karthik Kappaganthu</a><sup>‚Ä†</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://www.cs.cmu.edu/~./yiming/">Yiming Yang</a><sup>‚ô†</sup>, 
            </span>
            <span class="author-block"> 
              <a href="http://shyamupa.com/">Shyam Upadhyay</a><sup>‚Ä†</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://www.manaalfaruqui.com/">Manaal Faruqui</a><sup>‚Ä†</sup>, 
            </span>
            <span class="author-block"> 
              <a href="https://www.cse.iitd.ac.in/~mausam/">Mausam</a><sup>‚ô¢</sup> 
            </span> 
          </div>
          <div class="is-size-5 publication-authors"> 
            <span class="author-block"><sup>‚ô†</sup>Carnegie Mellon University</span>
            <span class="author-block"><sup>‚Ä†</sup>Google</span>
            <span class="author-block"><sup>‚Ä°</sup>Google DeepMind</span>
            <br>
            <span class="author-block"><sup>‚ô¢</sup>IIT Delhi</span>
            <span class="author-block"><sup>‚ô£</sup>University of Southern California</span>
            <br><br>
            <span class="author-block">‚àóEqual Contribution. Work started and partly done during Aman‚Äôs internship at Google.</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2310.12963.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <span class="link-block">
                <a href="https://github.com/automix-llm/automix/colabs" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <svg class="svg-inline--fa fa-terminal fa-w-20" aria-hidden="true" focusable="false" data-prefix="fas" data-icon="terminal" role="img" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 640 512"><path fill="currentColor" d="M257.981 272.971L63.638 467.314c-9.373 9.373-24.569 9.373-33.941 0L7.029 444.647c-9.357-9.357-9.375-24.522-.04-33.901L161.011 256 6.99 101.255c-9.335-9.379-9.317-24.544.04-33.901l22.667-22.667c9.373-9.373 24.569-9.373 33.941 0L257.981 239.03c9.373 9.372 9.373 24.568 0 33.941zM640 456v-32c0-13.255-10.745-24-24-24H312c-13.255 0-24 10.745-24 24v32c0 13.255 10.745 24 24 24h304c13.255 0 24-10.745 24-24z"></path></svg>
                    </span>
                    <span>Colab</span>
                </a>
            </span>

              <span class="link-block">
                <a href="https://github.com/automix-llm/automix"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>

              <span class="link-block">
                <a href="mailto:automix-models@googlegroups.com"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-envelope"></i>
                  </span>
                  <span>Email</span>
                </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<h2 class="subtitle has-text-centered">
  <span class="dnerf">TLDR:</span> AutoMix optimizes cost and accuracy by 
  routing queries between small and large language models,<br> through self-verification and POMDP routing.
</h2>

<section class="hero teaser">
  <div class="container is-max-desktop">
      <div class="hero-body">
          <figure class="image">
              <img id="animatedGif" src="static/images/automix_teaser.png">
          </figure>
      </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified is-size-4">
          <p>
            Large language models (LLMs) are now available from cloud API providers in various sizes and configurations. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present <b>AutoMix</b>, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix are two key technical contributions. First, it has a few-shot <i>self-verification</i> mechanism, which estimates the reliability of its own outputs without requiring extensive training. Second, given that self-verification can be noisy, it employs a <i>POMDP-based router</i> that effectively selects an appropriately sized model based on answer confidence. Experiments across five language models and five challenging datasets show that AutoMix consistently surpasses strong baselines, reducing computational cost by over <i>50%</i> for comparable performance.
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
  </div>
</section>

<section class="section" id="highlights">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
          <div class="column ">
            <h2 class="title is-3">Key-Highlights</h2>
            <div class="content has-text-justified is-size-4">
              <ul>
                <li>üîÅ <strong>Few-shot Self-Verification:</strong> Accurately assess correctness using just the context, without extensive training.</li>
                <li>üí° <strong>POMDP-based Router:</strong> Overcome noisy verification signals and dynamically select from multiple models.</li>
                <li>üìà <strong>Efficiency Gains:</strong> Achieve over 50% reduction in computational cost at similar performance.</li>
                <li>üîå <strong>Black-box Applicability:</strong> Works out-of-the-box with closed-source APIs, no access to weights required.</li>
              </ul>
            </div>
          </div>
      </div>
  </div>
</section>

<section class="section" id="results">
  <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
              <h2 class="title is-3">Results Summary</h2>
              <strong><span class="pal">AutoMix</span> consistently outperforms strong baselines, providing a better incremental benefit per cost (IBC) and reducing computational cost by over 50% while maintaining comparable performance.</strong>
              <br><br>
              <figure class="image is-64by64">
                  <img src="static/images/results_table.png">
              </figure>
              <figure class="image is-64by64">
                <img src="static/images/results_graph.png">
            </figure>
              <hr>
          </div>
      </div>
  </div>
</section>

<section class="section" id="library">
  <div class="container is-max-desktop">
    <div class="box has-background-light">
      <h2 class="title is-3">Using AutoMix in your code</h2>
      Using AutoMix in your code takes only a few lines of changes.

      <div class="box has-background-light">
        <div class="content has-text-justified is-size-6">
          <p><strong>1. Installing</strong></p>
          <pre class=" language-shell"><code class="language-shell">pip install automix-llm</code></pre>
        </div>
      </div>

      <div class="box has-background-light">
        <div class="content has-text-justified is-size-6">
          <p><strong>2. Training and Inference</strong></p>
          <pre class=" language-python"><code class="language-python">from automix import Automix, POMDP

mixer = Automix(POMDP(*args, **kwargs))
mixer.train(train_data)
results = mixer.evaluate(test_data)</code></pre>
        </div>
      </div>

      <div class="box has-background-light">
        <div class="content has-text-justified is-size-6">
          <p><strong>3. High Customizability</strong></p>
          Check out our <a href="https://github.com/automix-llm/automix">Github repo</a> for more details!
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@misc{aggarwal2024automixautomaticallymixinglanguage,
      title={AutoMix: Automatically Mixing Language Models}, 
      author={Pranjal Aggarwal and Aman Madaan and Ankit Anand and Srividya Pranavi Potharaju and Swaroop Mishra and Pei Zhou and Aditya Gupta and Dheeraj Rajagopal and Karthik Kappaganthu and Yiming Yang and Shyam Upadhyay and Manaal Faruqui and Mausam},
      year={2024},
      eprint={2310.12963},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2310.12963}, 
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="https://arxiv.org/pdf/2310.12963.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/automix-llm/automix" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Template adapted from <a href="http://nerfies.github.io/">Nerfies</a> by Keunhong Park et
            al. and uses <a href="https://bulma.io/">Bulma</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>