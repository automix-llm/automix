{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMvAKLNuVjbGEhEsrq7bEwc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/automix-llm/automix/blob/main/Scoring.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zv8AmsGUr1S2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Scripts for adding scores to outputs"
      ],
      "metadata": {
        "id": "Cqkpl0iSt8zW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def f1_score(prediction, ground_truth):\n",
        "    prediction_tokens = normalize_answer(prediction).split()\n",
        "    ground_truth_tokens = normalize_answer(ground_truth).split()\n",
        "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0\n",
        "    precision = 1.0 * num_same / len(prediction_tokens)\n",
        "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
        "    f1 = (2 * precision * recall) / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def normalize_answer(s):\n",
        "    \"\"\"Lower text and remove punctuation, articles, and extra whitespace.\"\"\"\n",
        "    def remove_articles(text):\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text):\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    def remove_punc(text):\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def lower(text):\n",
        "        return text.lower()\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "def calculate_f1_for_models(df, model_sizes, ground_truth_col='output'):\n",
        "    \"\"\"\n",
        "    Calculates F1 score for different model sizes and adds the results as new columns in the DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    - df (pd.DataFrame): The DataFrame containing prediction data.\n",
        "    - model_sizes (list of str): List containing strings that denote model sizes.\n",
        "      Used to create column names dynamically.\n",
        "    - ground_truth_col (str, optional): The name of the column containing ground truth data.\n",
        "      Defaults to 'output'.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: The original DataFrame with added columns for the F1 scores.\n",
        "    \"\"\"\n",
        "    for size in model_sizes:\n",
        "        pred_col = f'llama{size}_pred_ans'\n",
        "        f1_col = f'llama{size}_f1'\n",
        "        df[f1_col] = df.apply(\n",
        "            lambda r: f1_score(prediction=r[pred_col], ground_truth=r[ground_truth_col]),\n",
        "            axis=1\n",
        "        )\n",
        "    return df\n",
        "\n"
      ],
      "metadata": {
        "id": "6BU3413Lt-3z"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bKM6Yge1uAry"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### For quality, LLAMA2-13b sometimes generates only the option (e.g., a). Simple matching with output won't work, so we have to do map the generated option to the correct answer and do the matching.\n"
      ],
      "metadata": {
        "id": "OzayxP7luCOW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from typing import List\n",
        "\n",
        "def extract_option(row: pd.Series) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the correct option from the provided row.\n",
        "\n",
        "    Parameters:\n",
        "        row (pd.Series): A row of a DataFrame, expected to contain 'question' and 'output' columns.\n",
        "\n",
        "    Returns:\n",
        "        str: The letter of the correct option, or None if not found.\n",
        "    \"\"\"\n",
        "    options = re.findall(r'\\((\\w)\\) ([\\w\\s]+)', row['question'])\n",
        "    for option, value in options:\n",
        "        if value.strip() == row['output'].strip():\n",
        "            return option\n",
        "    return None\n",
        "\n",
        "def extract_option_from_prediction(pred: str) -> str:\n",
        "    \"\"\"\n",
        "    Extracts the selected option letter from a prediction string.\n",
        "\n",
        "    Parameters:\n",
        "        pred (str): The prediction string, expected to start with an option letter.\n",
        "\n",
        "    Returns:\n",
        "        str: The extracted option letter, or None if not found or if `pred` is empty.\n",
        "    \"\"\"\n",
        "    if len(pred.strip()) == 0:\n",
        "        return None\n",
        "\n",
        "    option = pred.split()[0]\n",
        "    for char in option:\n",
        "        if char in ['A', 'B', 'C', 'D']:\n",
        "            return char\n",
        "    return None\n",
        "\n",
        "def calculate_f1_for_multi_choice(df: pd.DataFrame, model_sizes: List[str], datasets: List[str]=[\"quality\"]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Computes F1 scores for predictions in multiple-choice format.\n",
        "\n",
        "    It extracts correct and predicted options and computes F1 scores, with special handling\n",
        "    for certain datasets. This function mutates the input DataFrame by adding new columns\n",
        "    for extracted options and possibly modifying F1 scores.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame containing prediction and ground truth data.\n",
        "            Expected to contain columns in the format 'llama{size}_pred_ans'.\n",
        "        model_sizes (List[str]): List of strings indicating the model sizes for which\n",
        "            predictions are available in `df` (e.g., ['13b', '70b']).\n",
        "        datasets (List[str], optional): List of dataset names that require special handling.\n",
        "            Defaults to [\"quality\"].\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: The original DataFrame with additional/modified columns for extracted\n",
        "            options and potentially modified F1 scores.\n",
        "    \"\"\"\n",
        "    df['correct_option'] = df.apply(extract_option, axis=1)\n",
        "\n",
        "    for size in model_sizes:\n",
        "        pred_ans_col = f'llama{size}_pred_ans'\n",
        "        pred_option_col = f'llama{size}_pred_option'\n",
        "        f1_col = f'llama{size}_f1'\n",
        "\n",
        "        # Remove single quotes from predictions for specified datasets\n",
        "        df[pred_ans_col] = df.apply(lambda r: r[pred_ans_col] if r[\"dataset\"] not in datasets else r[pred_ans_col].replace(\"'\", \"\"), axis=1)\n",
        "\n",
        "        # Extract the option from the prediction\n",
        "        df[pred_option_col] = df[pred_ans_col].apply(extract_option_from_prediction)\n",
        "\n",
        "        # Compute the F1 score: if dataset is in `datasets`, F1 is 1 if predicted option matches correct option, else it's 0\n",
        "        df[f1_col] = df.apply(lambda r: r[pred_option_col] == r['correct_option'] if r[\"dataset\"] in datasets else r[f1_col], axis=1)\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "UPmatQrTt_QN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B-EKHNlyuESl"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_predictions = pd.read_json(\"data/automix_llama2_outputs_ver_n32.jsonl\",\n",
        "                                       orient=\"records\", lines=True)"
      ],
      "metadata": {
        "id": "EeymtGkCuCvH"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_sizes = ['13b', '70b']\n",
        "\n",
        "# Calculating F1 scores for each model size\n",
        "inputs_with_predictions = calculate_f1_for_models(inputs_with_predictions, model_sizes)\n",
        "\n",
        "# Further processing and calculating F1 scores for multi-choice questions\n",
        "inputs_with_predictions = calculate_f1_for_multi_choice(inputs_with_predictions, model_sizes)"
      ],
      "metadata": {
        "id": "TqF98IOtuLaS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3FCms1AluavG"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_predictions = inputs_with_predictions[['id', 'pid', 'base_ctx', 'question', 'output', 'dataset',\n",
        "       'llama13b_pred_ans', 'llama70b_pred_ans', 'llama13b_ver', 'split',\n",
        "       'p_ver_13b', 'llama13b_ver_n32', 'p_ver_13b_n32', 'llama13b_f1',\n",
        "       'llama70b_f1']]"
      ],
      "metadata": {
        "id": "lvKNDhH7udjG"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs_with_predictions.groupby(\"dataset\")[['llama13b_f1', 'llama70b_f1']].mean()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "WhXRlXsbufPB",
        "outputId": "28850c33-c89e-4b60-fa3e-b42af220f286"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "              llama13b_f1  llama70b_f1\n",
              "dataset                               \n",
              "cnli             0.393899     0.553111\n",
              "coqa             0.478972     0.611645\n",
              "narrative_qa     0.205739     0.265013\n",
              "qasper           0.151614     0.286654\n",
              "quality          0.242174     0.325870"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>llama13b_f1</th>\n",
              "      <th>llama70b_f1</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>dataset</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>cnli</th>\n",
              "      <td>0.393899</td>\n",
              "      <td>0.553111</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>coqa</th>\n",
              "      <td>0.478972</td>\n",
              "      <td>0.611645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>narrative_qa</th>\n",
              "      <td>0.205739</td>\n",
              "      <td>0.265013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>qasper</th>\n",
              "      <td>0.151614</td>\n",
              "      <td>0.286654</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>quality</th>\n",
              "      <td>0.242174</td>\n",
              "      <td>0.325870</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "csNgkqB5uUDN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}